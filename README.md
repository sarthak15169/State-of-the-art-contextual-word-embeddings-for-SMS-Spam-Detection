# State-of-the-art-contextual-word-embeddings-for-SMS-Spam-Detection
Word embeddings are fundamental to most NLP and IR Problems. The most simple word embeddings do not take into account the context of a word. 
However, as is obvious, different words can mean different things in separate contexts. 
A recent approach to better word embeddings is contextual word embeddings. These use the context of a word to create an embedding for it. 
So, the same word in different contexts have different embeddings. 
Also, it is important to view words as a sequence of characters rather than a single block. 
This is exactly another one of the features of contextual word embeddings due to which they can cpture subword structures such as prefixes etc.
